{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["CFPkUVKGlwhp","DfNP-XrApvJF"],"gpuType":"L4","mount_file_id":"1o845tAR71HWgI8Pqx1daXW5_quoJ0oX3","authorship_tag":"ABX9TyO5XzkxfJTcBzf0MtVl8QJa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **TEAM: AI_Atlantique**"],"metadata":{"id":"lc9ovw-GoH5h"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGAKWWO1UDjC","executionInfo":{"status":"ok","timestamp":1733259718971,"user_tz":-60,"elapsed":251,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"aa8f4260-8f11-4a5e-f985-f63cd558dfb1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Dec  3 21:01:59 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   59C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["# **metric.py**"],"metadata":{"id":"CFPkUVKGlwhp"}},{"cell_type":"code","source":["from os.path import dirname, join\n","import numpy as np\n","import sofar\n","import torch\n","\n","class MeanSpectralDistortion:\n","    \"\"\"\n","    Metric Class used for evaluation, can also be used as loss function.\n","    MeanSpectalDistortion().get_spectral_distortion(ground_truth, predicted) for calculating error.\n","    \"\"\"\n","\n","    def __init__(self):\n","\n","        self.avg_hrir = sofar.read_sofa(join(dirname(__file__), 'data', 'Average_HRTFs.sofa'), verbose=False)\n","        self.source_positions = self.avg_hrir.SourcePosition\n","        self.elevation_index = self._get_elevation_index()\n","        self.weights = self._get_weights()\n","\n","    def _get_weights(self):\n","        \"\"\"\n","        This function load the weights which are used when you calculate the spectral distortion/ baseline predictions\n","        weights were calculated based on the paper \"Looking for a relevant similarity criterion fo HRTF clustering: a comparative study - Rozenn Nicol\".\n","\n","        Returns:\n","               normalized_weights: torch.tensor\n","\n","        \"\"\"\n","        # Generate a list of frequencies up to 24 kHz\n","        frequencies_Hz = np.linspace(0, 24000, 129)  # 129 points between 0 Hz and 24 kHz\n","        frequencies_kHz = frequencies_Hz / 1000\n","        inv_cb = 1 / (25 + 75 * (1 + 1.4 * frequencies_kHz**2) ** 0.69)  # inverse of delta (critical bandwidth)\n","        a0 = sum(inv_cb)\n","        normalized_weights = inv_cb / a0\n","        return normalized_weights\n","\n","    def _get_elevation_index(self):\n","        \"\"\"\n","        Helper function to get elevation indexes.\n","        Args:\n","            You can change the elevation range as required. We will use the elvation range between -30 to 30\n","            Returns:\n","             all index for the elevation range\"\"\"\n","        # this function gives the index of the directions for which you need to evaluate your results.\n","\n","        azimuths = self.source_positions[:, 0]\n","        elevations = self.source_positions[:, 1]\n","\n","        # Define the elevation range\n","        elevation_min = -30\n","        elevation_max = 30\n","        # Find the indices for the specific elevation range\n","        elevation_indices = np.where((elevations >= elevation_min) & (elevations <= elevation_max))[0]\n","\n","        # Ensure that elevation_indices is a NumPy array of integers\n","        return np.array(elevation_indices, dtype=int)\n","\n","    def get_spectral_distortion(self, hrtf_ground_truth: torch.Tensor, hrtf_predicted: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Computes the spectral distortion between the inputs.\n","\n","        Args:\n","            hrtf_ground_truth: torch.tensor\n","            hrtf_predicted: torch.tensor\n","        Returns:\n","            weighted_error: torch.tensor in dB\n","\n","        \"\"\"\n","\n","        weighted_error = ((torch.from_numpy(self.weights) * (hrtf_ground_truth[self.elevation_index].abs() - hrtf_predicted[self.elevation_index].abs())) ** 2).mean()\n","        return weighted_error.log10() * 10\n","\n","    def get_spectral_distortion_numpy(self, hrtf_ground_truth: np.ndarray, hrtf_predicted: np.ndarray) -> float:\n","        \"\"\"\n","        Computes the spectral distortion between the inputs.\n","\n","        Args:\n","            hrtf_ground_truth: np.ndarray\n","            hrtf_predicted: np.ndarray\n","        Returns:\n","            weighted_error: float\n","\n","        \"\"\"\n","\n","        weighted_error = np.mean(self.weights * (np.abs(hrtf_ground_truth[self.elevation_index]) - np.abs(hrtf_predicted[self.elevation_index])) ** 2)\n","        return float(10. * np.log10(weighted_error))"],"metadata":{"id":"06h1oDI9ld6v","executionInfo":{"status":"ok","timestamp":1733259741370,"user_tz":-60,"elapsed":3559,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **utils.py**"],"metadata":{"id":"_dI7uAhMpngZ"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WUQdR8FlDrV","executionInfo":{"status":"ok","timestamp":1733259958851,"user_tz":-60,"elapsed":205303,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"c3da1abf-4b8e-4966-f171-1cec50e6d518"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics\n"]},{"output_type":"stream","name":"stderr","text":["0it [00:07, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Image size: torch.Size([1, 2, 19, 1024, 1024]) and HRTF size: torch.Size([1, 19, 2, 129])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from __future__ import annotations\n","from typing import Dict, List, Tuple\n","import sofar\n","import glob\n","import numpy as np\n","from imageio.v3 import imread\n","import os\n","import torch\n","import tqdm\n","from torchvision.transforms import Compose, Resize, ToTensor\n","\n","\n","\n","all_tasks = [np.arange(19).tolist(), np.arange(19, step=3).tolist(), [3, 6, 9]]\n","\n","\n","class SonicomDatabase(torch.utils.data.Dataset):\n","\n","    def __init__(\n","        self,\n","        root_dir: str,\n","        hrtf_type=\"FreeFieldCompMinPhase\",\n","        no_itd=True,\n","        sampling_rate=\"48kHz\",\n","        nfft=256,\n","        training_data: bool = True,\n","        task_id: int = 0,\n","        folder_structure: str = 'v2'\n","    ):\n","        \"\"\"\n","        Args:\n","            root_dir: Directory with all the HRTF files in subfolder.\n","            hrtf_type: can be any of ['Raw','Windowed','FreeFieldComp','FreeFieldCompMinPhase'(default)]\n","            sampling_rate: any of 44kHz, 48kHz, 96kHz\n","            nfft: fft length\n","            training_data: if true then return training dataset\n","            task_id: task id determines how many images will be used for inference. Can be 0, 1, or 2.\n","        \"\"\"\n","        super().__init__()\n","        self.root_dir = root_dir\n","        self.hrtf_type = hrtf_type\n","        self.nfft = nfft\n","\n","        if no_itd:\n","            itd_str = \"NoITD_\"\n","        else:\n","            itd_str = \"\"\n","\n","        if folder_structure not in {'v1', 'v2'}:\n","            raise RuntimeError('Unknown folder structure version')\n","        pathname = f\"P*/P*/HRTF/HRTF/{sampling_rate}/*_{hrtf_type}_{itd_str}{sampling_rate}.sofa\" if folder_structure == 'v1' else \\\n","                   f\"SONICOM_HRTF/P*/HRTF/HRTF/{sampling_rate}/*_{hrtf_type}_{itd_str}{sampling_rate}.sofa\"\n","        self.hrtf_files = glob.glob(os.path.join(root_dir, pathname))\n","        # print('Found ' + str(len(self.hrtf_files)) + ' files')\n","\n","\n","\n","\n","        if training_data:\n","            self.image_dir = os.path.join(root_dir, \"SONICOM_TrainingData_pics\")\n","            self.task = all_tasks[0]\n","        else:\n","            print(os.path.join(root_dir, \"SONICOM_TestData_pics\"))\n","            self.image_dir = os.path.join(root_dir, \"SONICOM_TestData_pics\")\n","            self.task = all_tasks[task_id]\n","\n","        self.all_image_names = [i for i in os.listdir(self.image_dir) if \".png\" in i]\n","        self.all_subjects = self.get_available_ids()\n","\n","        # read one to get coordinate system information\n","        try:\n","            tmp = sofar.read_sofa(self.hrtf_files[0], verbose=False)\n","            self.training_data = training_data\n","            self.position = tmp.SourcePosition\n","        except (IndexError, ValueError):\n","            print(\"Check if Dataset is saved as described in the notebook.\")\n","            return None\n","\n","\n","    def __len__(self):\n","        return len(self.all_subjects)\n","\n","    def load_all_hrtfs(self) -> torch.Tensor:\n","        \"\"\"\n","        This function loads all the HRTFs from the list of IDs.\n","\n","        Returns:\n","            Magnitude Spectrum of HRTFs : torch.Tensor\n","        \"\"\"\n","        HRTFs = torch.zeros(\n","            (self.__len__(), self.position.shape[0], 2, self.nfft // 2 + 1)\n","        )\n","\n","        allids = np.unique([cur_id[:5] for cur_id in self.all_image_names])\n","        for idx in range(len(allids)):\n","            if allids[idx] == allids[idx - 1] and idx > 0:\n","                HRTFs[idx] = HRTFs[idx - 1]\n","            else:\n","                HRTFs[idx] = torch.from_numpy(\n","                    self.load_subject_id_hrtf(allids[idx])\n","                ).abs()\n","        return HRTFs\n","\n","    def load_image(self, image_name: str) -> Tuple[np.ndarray, str, str]:\n","        \"\"\"\n","        This function read all the image files in the directory, get the ID of the image, Left or Right side of the pinna.\n","\n","        Args:\n","            image_name (str): e.g. P0002_left_0.png\n","\n","        Returns:\n","            image: torch.Tensor\n","            ID: (str) Subject ID of the loaded image\n","            Face_Side: (str) If the image loaded is of the left ear or the right ear\n","        \"\"\"\n","\n","        image = imread(os.path.join(self.image_dir, image_name))\n","        ID = image_name[:5]\n","        Face_Side = [\"left\" if \"left\" in image_name else \"right\"]\n","\n","        return image, ID, Face_Side\n","\n","    def get_image_names_from_id(self, id: str) -> List[str]:\n","        \"\"\"\n","        This function helps to get the image names from the directory.\n","\n","        Args:\n","            id (str): Subject ID e.g. 'P0001'\n","        Returns:\n","            List of image name\n","        \"\"\"\n","        return [\n","            x for x in os.listdir(self.image_dir) if f\"{id}\" in x\n","        ]  # glob.glob(os.path.join(self.image_dir, f'{id}*'))\n","\n","    def get_available_ids(self) -> List[str]:\n","        \"\"\"\n","        This function returns all unique IDs from the list of images.\n","\n","        Args:\n","            all_images (list of str)\n","        Returns:\n","            list of unique IDs\n","        \"\"\"\n","        return list({name[:5] for name in self.all_image_names})\n","\n","    def _extract_number_of_image(self, image_name: str) -> List[int]:\n","        \"\"\"\n","        Extracts the image number of the subject from an image filename.\n","\n","        Args:\n","            image_name (str): Filename of the image.\n","\n","        Returns:\n","            Optional[int]: value if successfully extracted; otherwise, None.\n","        \"\"\"\n","        try:\n","            azi_str = image_name.split(\"t_\")[1]\n","            number = int(azi_str.split(\".\")[0])\n","            return number\n","        except (IndexError, ValueError):\n","            return None\n","\n","    def _get_task_subset_image_names(self, image_names: List[str]) -> Tuple[List[str], List[str]]:\n","        \"\"\"\n","        Returns two Lists of left and right image names from selected subset (based on task).\n","\n","        Args:\n","            image_names (List of str): Filenames of the images.\n","\n","        Returns:\n","            Dict e.g. {left_0: [0, 'P0002_left_0.png'], right_0: [1, 'P0002_right_0.png']}\n","        \"\"\"\n","\n","        left_names = []\n","        right_names = []\n","        for i in image_names:\n","            cur_azi = self._extract_number_of_image(i)\n","            if cur_azi in self.task:\n","                if \"left\" in i:\n","                    left_names.append(i)  # channel, name\n","                if \"right\" in i:\n","                    right_names.append(i)\n","\n","        return left_names, right_names\n","\n","    def get_all_images_and_HRTF_from_id(self, id: str) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Loads all the images for the subject (only the subset) and the corresponding HRTF.\n","\n","        Args:\n","            ID of each subject (str): e.g. P0001\n","\n","        Returns:\n","            all_images: torch.Tensor of shape (ear_idx, image_idx, height, width)\n","            HRTFs: torch.Tensor\n","        \"\"\"\n","        image_names = self.get_image_names_from_id(id)\n","        image_names.sort()\n","        left_images_filenames, right_images_filenames = (\n","            self._get_task_subset_image_names(image_names)\n","        )\n","        left_images = []\n","        right_images = []\n","\n","        if not left_images_filenames or not right_images_filenames:\n","            raise FileNotFoundError(f\"No images found for subject ID '{id}'.\")\n","\n","        left_images = torch.from_numpy(np.stack([imread(os.path.join(self.image_dir, path)) for path in left_images_filenames]))\n","        right_images = torch.from_numpy(np.stack([imread(os.path.join(self.image_dir, path)) for path in right_images_filenames]))\n","\n","        all_images = torch.stack((left_images, right_images))\n","\n","        HRTF = self.load_subject_id_hrtf(id)\n","\n","        return all_images, HRTF\n","\n","    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        This function is used by the Dataloader, it iterates through the number of subjects in the current dataset\n","        and provides the corresponding Images, HRTFs and Subject IDs.\n","        \"\"\"\n","\n","        id = self.all_subjects[idx]\n","        all_images, HRTF = self.get_all_images_and_HRTF_from_id(id)\n","\n","        return all_images, HRTF\n","\n","\n","    def load_subject_id_hrtf(self, subject_id: str, return_sofa: bool = False) -> np.ndarray:\n","        hrtf_file = [s for s in self.hrtf_files if subject_id + \"_\" + self.hrtf_type in s][0]\n","        if not hrtf_file:\n","            print(subject_id + \" Not found!\")\n","            return None\n","\n","        data = sofar.read_sofa(hrtf_file, verbose=False)\n","        if return_sofa:\n","            return data\n","\n","        # Filtrer pour garder seulement les angles dans self.task\n","        filtered_hrir = data.Data_IR[self.task]  # Indices des angles définis dans self.task\n","        return self._compute_HRTF(filtered_hrir)\n","\n","\n","    def _load_hrir(self, hrtf_file: sofar.Sofa) -> np.ndarray:\n","        \"\"\"\n","        This function load the HRIR data for the given filename.\n","\n","        Args:\n","              sofa file\n","            Returns:\n","               HRIR data\"\"\"\n","        data = sofar.read_sofa(hrtf_file, verbose=False)\n","        return data.Data_IR\n","\n","    def _compute_HRTF(self, hrir: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        This function compute the RFFT of the given HRIRs and return HRTFs.\n","\n","        Args:\n","              HRIRs (time domain)\n","            Returns:\n","               HRTFs (Frequency domain)\"\"\"\n","\n","        return np.fft.rfft(hrir, n=self.nfft)\n","\n","\n","def baseline_spectral_distortion(sd: SonicomDatabase, path_to_baseline_hrtf: str = \"/content/drive/MyDrive/TechArena/TechArena20241120/data/Average_HRTFs.sofa\") -> float:\n","    # this function calculate the spectral difference as mean square error between your ground truth HRTFs and the baseline average HRTFs\n","    # load all HRTFS, concat in 1 tensor, clone Average_HRTFs as many times and then find get_spectral_distortion\n","    \"\"\"Returns:\n","    baseline prediction MSE in dB\n","    \"\"\"\n","\n","    all_HRTFs = sd.load_all_hrtfs()\n","    baseline_HRIR = sofar.read_sofa(path_to_baseline_hrtf, verbose=False).Data_IR\n","    baseline_HRTF = torch.from_numpy(sd._compute_HRTF(baseline_HRIR))\n","    baseline_HRTF = baseline_HRTF.unsqueeze(0).repeat(all_HRTFs.shape[0], 1, 1, 1)\n","    eval_metric = MeanSpectralDistortion()\n","\n","    return eval_metric.get_spectral_distortion(all_HRTFs, baseline_HRTF)\n","\n","\n","def convert_to_HRIR(hrtfs: np.ndarray) -> np.ndarray:\n","    return np.fft.irfft(hrtfs, axis=-1)\n","\n","def save_sofa(HRIR: np.ndarray, output_path: str, reference_sofa: sofar.Sofa):\n","    \"\"\"\n","    Save the HRIR to a SOFA object file. See main() for example usage\n","\n","    Args:\n","        HRIR (np.ndarray): HRIR of shape (793, 2, 256).\n","        output_path (str): Path where the SOFA file will be saved.\n","        reference_sofa (str): The SOFA object to copy information\n","    \"\"\"\n","    hrtf = reference_sofa\n","    hrtf.Data_IR = HRIR\n","    sofar.write_sofa(output_path, hrtf, 0)\n","\n","\n","if __name__ == \"__main__\":\n","    from torch.utils.data import DataLoader\n","\n","    sonicom_root = \"/content/drive/MyDrive/TechArena/TechArena20241120/data\"\n","    sd = SonicomDatabase(sonicom_root, training_data=False, task_id=0)\n","    train_dataloader = DataLoader(sd, batch_size=1, shuffle=False)\n","\n","    for i, (images, hrtf) in tqdm.tqdm(enumerate(train_dataloader)):\n","        print(f\"Image size: {images.shape} and HRTF size: {hrtf.shape}\")\n","        break\n","\n","    # Error = baseline_spectral_distortion(sd)\n","    # print(Error)"]},{"cell_type":"markdown","source":["# **model.py**\n","\n"],"metadata":{"id":"DfNP-XrApvJF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import gc\n","from torch.utils.data import DataLoader\n","\n","# Modèle d'encodage\n","class PinnaEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.25)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.dropout(x)\n","        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","        x = self.dropout(x)\n","        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n","        x = self.dropout(x)\n","        return x\n","\n","# Générateur HRTF\n","\n","class HRTFGenerator(nn.Module):\n","    def __init__(self, num_angles=19, num_freq_bins=129):\n","        super().__init__()\n","        self.encoder = PinnaEncoder()\n","        self.flatten_size = None\n","        self.fc1 = None\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, num_angles * num_freq_bins * 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, images):\n","        images = images.squeeze(3).float()  # Assurez-vous que les images sont en float32\n","\n","        batch_size, num_ears, num_views, height, width = images.shape\n","        features = []\n","        for ear in range(num_ears):\n","            ear_features = []\n","            for view in range(num_views):\n","                x = images[:, ear, view, :, :].unsqueeze(1)\n","                x = self.encoder(x)\n","                ear_features.append(x)\n","            ear_features = torch.stack(ear_features, dim=1)\n","            ear_features = torch.mean(ear_features, dim=1)\n","            features.append(ear_features)\n","        features = torch.cat(features, dim=1)\n","\n","\n","        if self.flatten_size is None:\n","            self.flatten_size = features.view(batch_size, -1).shape[1]\n","            self.fc1 = nn.Linear(self.flatten_size, 512).to(features.device)\n","\n","        features = features.view(batch_size, -1)\n","        x = F.relu(self.fc1(features))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        hrtf = x.view(batch_size, 19, 2, 129)\n","        return hrtf\n","\n","# Entraîneur\n","class HRTFTrainer:\n","    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","        self.model = model.to(device)\n","        self.device = device\n","        self.criterion = nn.MSELoss()\n","        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    def train_epoch(self, dataloader, accumulation_steps=4):\n","        self.model.train()\n","        total_loss = 0\n","        for i, (images, hrtfs) in enumerate(dataloader):\n","            images = images.to(self.device).float()  # Conversion explicite en float32\n","            hrtfs = hrtfs.to(self.device).float()  # Conversion explicite en float32\n","\n","            # Convertir les données complexes en réels\n","            hrtfs_real = torch.abs(hrtfs)\n","\n","            self.optimizer.zero_grad()\n","            predictions = self.model(images)\n","\n","            # Conversion des prédictions en réels\n","            predictions_real = torch.abs(predictions)\n","\n","            # Calcul de la perte\n","            loss = self.criterion(predictions_real, hrtfs_real)\n","            loss = loss / accumulation_steps\n","            loss.backward()\n","\n","            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","\n","            total_loss += loss.item()\n","            del images, hrtfs, predictions\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","        return total_loss / len(dataloader)\n","\n","    def validate(self, dataloader):\n","        self.model.eval()\n","        total_loss = 0\n","        with torch.no_grad():\n","            for images, hrtfs in dataloader:\n","                images = images.to(self.device).float()  # Conversion explicite en float32\n","                hrtfs = hrtfs.to(self.device).float()  # Conversion explicite en float32\n","\n","                # Convertir les données complexes en réels\n","                hrtfs_real = torch.abs(hrtfs)\n","\n","                predictions = self.model(images)\n","\n","                # Conversion des prédictions en réels\n","                predictions_real = torch.abs(predictions)\n","\n","                # Calcul de la perte\n","                loss = self.criterion(predictions_real, hrtfs_real)\n","                total_loss += loss.item()\n","\n","                del images, hrtfs, predictions\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","        return total_loss / len(dataloader)\n","\n","\n","# Entraînement du modèle\n","def train_model(train_loader, val_loader, num_epochs=50):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = HRTFGenerator().to(device)\n","    trainer = HRTFTrainer(model)\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch+1}/{num_epochs}:')\n","        train_loss = trainer.train_epoch(train_loader)\n","        val_loss = trainer.validate(val_loader)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), 'best_model.pth')\n","\n","        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    print(\"Entraînement terminé.\")\n","    return model\n"],"metadata":{"id":"saPJ_sPURhN9","executionInfo":{"status":"ok","timestamp":1733259992078,"user_tz":-60,"elapsed":330,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Sol.py**"],"metadata":{"id":"e-R3DmgM6EvE"}},{"cell_type":"code","source":["from torchvision import transforms\n","\n","# Ajout du prétraitement\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),            # Conversion en image PIL (nécessaire pour torchvision)\n","    transforms.Resize((128, 128)),      # Redimensionnement des images\n","    transforms.ToTensor(),              # Conversion en tenseur\n","    transforms.Normalize((0.5,), (0.5,))  # Normalisation (valeurs moyennes et écarts-types)\n","])\n","\n","# Modification de la méthode __getitem__ pour inclure le transform\n","class ModifiedSonicomDatabase(SonicomDatabase):\n","    def __init__(self, *args, transform=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.transform = transform\n","\n","    def __getitem__(self, idx: int):\n","        id = self.all_subjects[idx]\n","        all_images, HRTF = self.get_all_images_and_HRTF_from_id(id)\n","\n","\n","       # Appliquer la transformation à chaque image\n","        transformed_images = []\n","        for ear_images in all_images:  # Parcours des oreilles (gauche et droite)\n","            transformed_ear = torch.stack([self.transform(img.numpy()) for img in ear_images])\n","            transformed_images.append(transformed_ear)\n","\n","        # Empiler les images transformées pour former un tenseur\n","        all_images = torch.stack(transformed_images)  # [2, num_views, 128, 128]\n","\n","       # Supprimer les dimensions inutiles si nécessaire\n","        all_images = all_images.squeeze(3)  # Supprime la dimension 1 s'il y en a une inutile.\n","        return all_images, HRTF\n","\n","\n","\n","# Utilisation de la classe modifiée\n","train_data = ModifiedSonicomDatabase(\n","    \"/content/drive/MyDrive/TechArena/TechArena20241120/data\",\n","    training_data=True,\n","    transform=transform\n",")\n","\n","val_data = ModifiedSonicomDatabase(\n","    \"/content/drive/MyDrive/TechArena/TechArena20241120/data\",\n","    training_data=False,\n","    transform=transform\n",")\n","\n","train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoVcdINpwCRz","executionInfo":{"status":"ok","timestamp":1733260009518,"user_tz":-60,"elapsed":13212,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"2dc5d56d-c957-4b96-b0c5-5ffd32ac93f9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics\n"]}]},{"cell_type":"code","source":["# # from torch.utils.data import DataLoader\n","# # from utils import SonicomDatabase\n","# # from model import train_model\n","\n","# # Create dataloaders\n","# train_data = SonicomDatabase(\"/content/drive/MyDrive/TechArena/TechArena20241120/data\", training_data=True)\n","# val_data = SonicomDatabase(\"/content/drive/MyDrive/TechArena/TechArena20241120/data\", training_data=False)\n","# train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n","# val_loader = DataLoader(val_data, batch_size=1)\n"],"metadata":{"id":"BH3u2FnpqQuH","executionInfo":{"status":"ok","timestamp":1733157385250,"user_tz":-60,"elapsed":7,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","model = train_model(train_loader, val_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rs8_roxXSRhh","executionInfo":{"status":"ok","timestamp":1733265927299,"user_tz":-60,"elapsed":5906758,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"ad49a406-be2a-443a-81ff-67fe0202de69"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50:\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-6-b61c02631857>:84: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)\n","  hrtfs = hrtfs.to(self.device).float()  # Conversion explicite en float32\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 - Train Loss: 0.0060, Val Loss: 0.0220\n","Epoch 2/50:\n","Epoch 2/50 - Train Loss: 0.0044, Val Loss: 0.0149\n","Epoch 3/50:\n","Epoch 3/50 - Train Loss: 0.0034, Val Loss: 0.0117\n","Epoch 4/50:\n","Epoch 4/50 - Train Loss: 0.0029, Val Loss: 0.0088\n","Epoch 5/50:\n","Epoch 5/50 - Train Loss: 0.0027, Val Loss: 0.0104\n","Epoch 6/50:\n","Epoch 6/50 - Train Loss: 0.0027, Val Loss: 0.0076\n","Epoch 7/50:\n","Epoch 7/50 - Train Loss: 0.0025, Val Loss: 0.0100\n","Epoch 8/50:\n","Epoch 8/50 - Train Loss: 0.0025, Val Loss: 0.0077\n","Epoch 9/50:\n","Epoch 9/50 - Train Loss: 0.0022, Val Loss: 0.0079\n","Epoch 10/50:\n","Epoch 10/50 - Train Loss: 0.0022, Val Loss: 0.0084\n","Epoch 11/50:\n","Epoch 11/50 - Train Loss: 0.0023, Val Loss: 0.0061\n","Epoch 12/50:\n","Epoch 12/50 - Train Loss: 0.0023, Val Loss: 0.0059\n","Epoch 13/50:\n","Epoch 13/50 - Train Loss: 0.0023, Val Loss: 0.0094\n","Epoch 14/50:\n","Epoch 14/50 - Train Loss: 0.0021, Val Loss: 0.0074\n","Epoch 15/50:\n","Epoch 15/50 - Train Loss: 0.0020, Val Loss: 0.0064\n","Epoch 16/50:\n","Epoch 16/50 - Train Loss: 0.0020, Val Loss: 0.0065\n","Epoch 17/50:\n","Epoch 17/50 - Train Loss: 0.0020, Val Loss: 0.0069\n","Epoch 18/50:\n","Epoch 18/50 - Train Loss: 0.0021, Val Loss: 0.0057\n","Epoch 19/50:\n","Epoch 19/50 - Train Loss: 0.0020, Val Loss: 0.0062\n","Epoch 20/50:\n","Epoch 20/50 - Train Loss: 0.0018, Val Loss: 0.0069\n","Epoch 21/50:\n","Epoch 21/50 - Train Loss: 0.0018, Val Loss: 0.0058\n","Epoch 22/50:\n","Epoch 22/50 - Train Loss: 0.0018, Val Loss: 0.0058\n","Epoch 23/50:\n","Epoch 23/50 - Train Loss: 0.0018, Val Loss: 0.0072\n","Epoch 24/50:\n","Epoch 24/50 - Train Loss: 0.0019, Val Loss: 0.0058\n","Epoch 25/50:\n","Epoch 25/50 - Train Loss: 0.0018, Val Loss: 0.0056\n","Epoch 26/50:\n","Epoch 26/50 - Train Loss: 0.0017, Val Loss: 0.0053\n","Epoch 27/50:\n","Epoch 27/50 - Train Loss: 0.0017, Val Loss: 0.0058\n","Epoch 28/50:\n","Epoch 28/50 - Train Loss: 0.0017, Val Loss: 0.0055\n","Epoch 29/50:\n","Epoch 29/50 - Train Loss: 0.0017, Val Loss: 0.0057\n","Epoch 30/50:\n","Epoch 30/50 - Train Loss: 0.0018, Val Loss: 0.0063\n","Epoch 31/50:\n","Epoch 31/50 - Train Loss: 0.0017, Val Loss: 0.0060\n","Epoch 32/50:\n","Epoch 32/50 - Train Loss: 0.0016, Val Loss: 0.0059\n","Epoch 33/50:\n","Epoch 33/50 - Train Loss: 0.0017, Val Loss: 0.0062\n","Epoch 34/50:\n","Epoch 34/50 - Train Loss: 0.0017, Val Loss: 0.0053\n","Epoch 35/50:\n","Epoch 35/50 - Train Loss: 0.0017, Val Loss: 0.0060\n","Epoch 36/50:\n","Epoch 36/50 - Train Loss: 0.0016, Val Loss: 0.0054\n","Epoch 37/50:\n","Epoch 37/50 - Train Loss: 0.0016, Val Loss: 0.0055\n","Epoch 38/50:\n","Epoch 38/50 - Train Loss: 0.0016, Val Loss: 0.0062\n","Epoch 39/50:\n","Epoch 39/50 - Train Loss: 0.0016, Val Loss: 0.0053\n","Epoch 40/50:\n","Epoch 40/50 - Train Loss: 0.0016, Val Loss: 0.0054\n","Epoch 41/50:\n","Epoch 41/50 - Train Loss: 0.0016, Val Loss: 0.0054\n","Epoch 42/50:\n","Epoch 42/50 - Train Loss: 0.0016, Val Loss: 0.0055\n","Epoch 43/50:\n","Epoch 43/50 - Train Loss: 0.0016, Val Loss: 0.0054\n","Epoch 44/50:\n","Epoch 44/50 - Train Loss: 0.0016, Val Loss: 0.0055\n","Epoch 45/50:\n","Epoch 45/50 - Train Loss: 0.0016, Val Loss: 0.0056\n","Epoch 46/50:\n","Epoch 46/50 - Train Loss: 0.0016, Val Loss: 0.0058\n","Epoch 47/50:\n","Epoch 47/50 - Train Loss: 0.0015, Val Loss: 0.0057\n","Epoch 48/50:\n","Epoch 48/50 - Train Loss: 0.0016, Val Loss: 0.0053\n","Epoch 49/50:\n","Epoch 49/50 - Train Loss: 0.0015, Val Loss: 0.0052\n","Epoch 50/50:\n","Epoch 50/50 - Train Loss: 0.0015, Val Loss: 0.0054\n","Entraînement terminé.\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/TechArena/TechArena20241120/best_model.pth\")"],"metadata":{"id":"gUL8lGzV0Wwh","executionInfo":{"status":"ok","timestamp":1733268036297,"user_tz":-60,"elapsed":625,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# inferance.py"],"metadata":{"id":"kLqNs0wM08FQ"}},{"cell_type":"code","source":["!python /content/drive/MyDrive/TechArena/TechArena20241120/inference.py -l /content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics/P0002_left_0.png /content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics/P0002_left_1.png -r /content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics/P0002_right_0.png /content/drive/MyDrive/TechArena/TechArena20241120/data/SONICOM_TestData_pics/P0002_right_1.png -o /content/drive/MyDrive/TechArena/TechArena20241120/data/output/prediction.sofa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zn8xQVCB0TkU","executionInfo":{"status":"ok","timestamp":1733271167572,"user_tz":-60,"elapsed":22984,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"9ce36fa9-3acc-4cf6-e0c3-7c8b7dff0e0a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TechArena/TechArena20241120/inference.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=device)\n","Saved HRTF to /content/drive/MyDrive/TechArena/TechArena20241120/data/output/prediction.sofa\n"]}]},{"cell_type":"markdown","source":["# **Installation**"],"metadata":{"id":"GkKf5dXtqA3_"}},{"cell_type":"code","source":["!pip install sofar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD1GNIS3lTfo","executionInfo":{"status":"ok","timestamp":1733259731837,"user_tz":-60,"elapsed":4135,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"5dc9e1f3-0e9c-4b91-b6f4-aa92030bc70d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sofar\n","  Downloading sofar-1.2.0-py2.py3-none-any.whl.metadata (4.1 kB)\n","Collecting netCDF4 (from sofar)\n","  Downloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from sofar) (1.26.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from sofar) (4.12.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sofar) (2.32.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sofar) (24.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->sofar) (2.6)\n","Collecting cftime (from netCDF4->sofar)\n","  Downloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4->sofar) (2024.8.30)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sofar) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sofar) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sofar) (2.2.3)\n","Downloading sofar-1.2.0-py2.py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: cftime, netCDF4, sofar\n","Successfully installed cftime-1.6.4.post1 netCDF4-1.7.2 sofar-1.2.0\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"qLGrcp9-lWP1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733070441793,"user_tz":-60,"elapsed":488,"user":{"displayName":"MOHAMMED DAHBANI","userId":"15400768686088509552"}},"outputId":"1d7e5774-9ff4-4bd7-f1dc-4b0879afdb94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec  1 16:27:20 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"F4wH5jn6WnDY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Corbeille**"],"metadata":{"id":"cvLCh8QF4_sw"}},{"cell_type":"code","source":["# def load_subject_id_hrtf(self, subject_id: str, return_sofa: bool = False) -> sofar.Sofa | np.ndarray:\n","    #     \"\"\"\n","    #     This function load the HRIR data for the given file name and compute the RFFT of the HRIRs then return HRTFs\n","    #     Example if the file name is P0001, this function will load the sofa file of P0001 - read it and return the HRTF data of the P001\n","\n","    #     Args:\n","    #         subject_id (str): e.g. P0001, ..., P0200\n","    #     \"\"\"\n","\n","    #     hrtf_file = [s for s in self.hrtf_files if subject_id + \"_\" + self.hrtf_type in s][0]\n","    #     if not hrtf_file:\n","    #         print(subject_id + \" Not found!\")\n","    #         return None\n","    #     if return_sofa:\n","    #         return sofar.read_sofa(hrtf_file, verbose=False)\n","    #     else:\n","    #         hrir = self._load_hrir(hrtf_file)\n","    #         return self._compute_HRTF(hrir)"],"metadata":{"id":"2F5Z-1cKXJ30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import gc\n","# from torch.utils.data import DataLoader\n","# from torchvision import transforms\n","\n","# # Modèle d'encodage\n","# class PinnaEncoder(nn.Module):\n","#     def __init__(self):\n","#         super().__init__()\n","#         self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","#         self.bn1 = nn.BatchNorm2d(32)\n","#         self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","#         self.bn2 = nn.BatchNorm2d(64)\n","#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","#         self.bn3 = nn.BatchNorm2d(128)\n","#         self.pool = nn.MaxPool2d(2, 2)\n","#         self.dropout = nn.Dropout(0.25)\n","\n","#     def forward(self, x):\n","#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","#         x = self.dropout(x)\n","#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","#         x = self.dropout(x)\n","#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n","#         x = self.dropout(x)\n","#         return x\n","\n","# # Générateur HRTF\n","# class HRTFGenerator(nn.Module):\n","#     def __init__(self, num_angles=19, num_freq_bins=129):\n","#         super().__init__()\n","#         self.encoder = PinnaEncoder()\n","#         # self.flatten_size = 128 * 16 * 16  # Ajusté pour image 128x128\n","#         # self.fc1 = nn.Linear(self.flatten_size, 512)\n","#         # self.fc2 = nn.Linear(512, 256)\n","#         # self.fc3 = nn.Linear(256, num_angles * num_freq_bins * 2)  # *2 pour gauche et droite\n","#         # self.dropout = nn.Dropout(0.5)\n","#         self.fc1 = nn.Linear(65536, 512)  # Taille ajustée\n","#         self.fc2 = nn.Linear(512, 256)\n","#         self.fc3 = nn.Linear(256, 2 * 129)\n","#         self.dropout = nn.Dropout(0.5)\n","\n","#     def forward(self, images):\n","\n","#         if images.shape[3] == 1:\n","#           images = images.squeeze(3)\n","\n","#         batch_size, num_ears, num_views, height, width = images.shape\n","#         features = []\n","#         for ear in range(num_ears):\n","#             ear_features = []\n","#             for view in range(num_views):\n","#                 x = images[:, ear, view, :, :].unsqueeze(1)\n","#                 x = self.encoder(x)\n","#                 ear_features.append(x)\n","#             ear_features = torch.stack(ear_features, dim=1)\n","#             ear_features = torch.mean(ear_features, dim=1)\n","#             features.append(ear_features)\n","#         features = torch.cat(features, dim=1)\n","#         features = features.view(batch_size, -1)\n","#         print(f\"Shape before fc1: {features.shape}\")\n","#         # x = features.view(batch_size, -1)\n","#         # x = F.relu(self.fc1(x))\n","#         x = F.relu(self.fc1(features))\n","#         x = self.dropout(x)\n","#         x = F.relu(self.fc2(x))\n","#         x = self.dropout(x)\n","#         x = self.fc3(x)\n","#         hrtf = x.view(batch_size, 793, 2, 129)\n","#         return hrtf\n","\n","\n","# # Entraîneur\n","# class HRTFTrainer:\n","#     def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","#         self.model = model.to(device)\n","#         self.device = device\n","#         self.criterion = nn.MSELoss()\n","#         self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","#     def train_epoch(self, dataloader, accumulation_steps=4):\n","#         self.model.train()\n","#         total_loss = 0\n","#         for i, (images, hrtfs) in enumerate(dataloader):\n","#             images = images.to(self.device).float()\n","#             hrtfs = hrtfs.to(self.device)\n","#             self.optimizer.zero_grad()\n","\n","#             predictions = self.model(images)\n","#             loss = self.criterion(predictions, hrtfs)\n","#             loss = loss / accumulation_steps  # Accumulation des gradients\n","#             loss.backward()\n","\n","#             if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n","#                 self.optimizer.step()\n","#                 self.optimizer.zero_grad()\n","\n","#             total_loss += loss.item()\n","#             del images, hrtfs, predictions\n","#             gc.collect()\n","#             torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)\n","\n","#     def validate(self, dataloader):\n","#         self.model.eval()\n","#         total_loss = 0\n","#         with torch.no_grad():\n","#             for images, hrtfs in dataloader:\n","#                 images = images.to(self.device).float()\n","#                 hrtfs = hrtfs.to(self.device)\n","#                 predictions = self.model(images)\n","#                 loss = self.criterion(predictions, hrtfs)\n","#                 total_loss += loss.item()\n","#                 del images, hrtfs, predictions\n","#                 gc.collect()\n","#                 torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)\n","\n","# # # Entraînement du modèle\n","# # def train_model(train_loader, val_loader, num_epochs=50, batch_size=1):\n","# #     model = HRTFGenerator()\n","# #     trainer = HRTFTrainer(model)\n","# #     best_val_loss = float('inf')\n","\n","# #     for epoch in range(num_epochs):\n","# #         print(f'Epoch {epoch+1}/{num_epochs}:')\n","# #         train_loss = trainer.train_epoch(train_loader)\n","# #         val_loss = trainer.validate(val_loader)\n","\n","# #         if val_loss < best_val_loss:\n","# #             best_val_loss = val_loss\n","# #             torch.save(model.state_dict(), 'best_model.pth')\n","\n","# #         print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","# #         gc.collect()\n","# #         torch.cuda.empty_cache()\n","# #     return model\n","\n","# import torch\n","# import gc  # Garbage collector pour nettoyer la mémoire GPU\n","\n","# # Entraînement du modèle\n","# def train_model(train_loader, val_loader, num_epochs=50, batch_size=1):\n","#     model = HRTFGenerator().to(device)  # Transférer le modèle sur l'appareil\n","#     trainer = HRTFTrainer(model)\n","#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","#     best_val_loss = float('inf')\n","\n","#     for epoch in range(num_epochs):\n","#         print(f'Epoch {epoch+1}/{num_epochs}:')\n","#         model.train()  # Mettre le modèle en mode entraînement\n","#         train_loss = 0.0\n","\n","#         # Entraînement\n","#         for images, target in train_loader:\n","#             images = images.to(device)\n","#             target = target.to(device)\n","\n","#             # Gérer les dimensions si besoin\n","#             if target.shape != images.shape:\n","#                 print(f\"Mismatch de dimensions : images {images.shape} vs target {target.shape}\")\n","#                 target = target[:, :images.shape[1], :, :]\n","\n","#             # Gérer les données complexes\n","#             if torch.is_complex(images) or torch.is_complex(target):\n","#                 images = torch.cat([images.real, images.imag], dim=1)\n","#                 target = torch.cat([target.real, target.imag], dim=1)\n","\n","#             # Forward pass\n","#             optimizer.zero_grad()\n","#             output = model(images)\n","\n","#             # Vérification des dimensions de sortie\n","#             if output.shape != target.shape:\n","#                 print(f\"Output mismatch : output {output.shape}, target {target.shape}\")\n","#                 target = target[:, :output.shape[1], :, :]\n","\n","#             loss = torch.nn.functional.mse_loss(output, target)\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             train_loss += loss.item()\n","\n","#         train_loss /= len(train_loader)\n","#         print(f'Train Loss: {train_loss:.4f}')\n","\n","#         # Validation\n","#         model.eval()  # Mode évaluation\n","#         val_loss = 0.0\n","#         with torch.no_grad():\n","#             for images, target in val_loader:\n","#                 images = images.to(device)\n","#                 target = target.to(device)\n","\n","#                 if torch.is_complex(images) or torch.is_complex(target):\n","#                     images = torch.cat([images.real, images.imag], dim=1)\n","#                     target = torch.cat([target.real, target.imag], dim=1)\n","\n","#                 output = model(images)\n","\n","#                 if output.shape != target.shape:\n","#                     target = target[:, :output.shape[1], :, :]\n","\n","#                 val_loss += torch.nn.functional.mse_loss(output, target).item()\n","\n","#         val_loss /= len(val_loader)\n","#         print(f'Val Loss: {val_loss:.4f}')\n","\n","#         # Sauvegarder le meilleur modèle\n","#         if val_loss < best_val_loss:\n","#             best_val_loss = val_loss\n","#             torch.save(model.state_dict(), 'best_model.pth')\n","\n","#         # Nettoyer la mémoire\n","#         gc.collect()\n","#         torch.cuda.empty_cache()\n","\n","#     print(\"Entraînement terminé.\")\n","#     return model"],"metadata":{"id":"TAKqqhWp5VG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Entraîneur\n","# class HRTFTrainer:\n","#     def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","#         self.model = model.to(device)\n","#         self.device = device\n","#         self.criterion = nn.MSELoss()\n","#         self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","#     def train_epoch(self, dataloader, accumulation_steps=4):\n","#         self.model.train()\n","#         total_loss = 0\n","#         for i, (images, hrtfs) in enumerate(dataloader):\n","#             images = images.to(self.device).float()\n","#             hrtfs = hrtfs.to(self.device)\n","\n","#             # Convertir les données complexes en réels\n","#             hrtfs_real = torch.abs(hrtfs)  # Utilisation de la magnitude\n","\n","#             self.optimizer.zero_grad()\n","#             predictions = self.model(images)\n","\n","#             # Conversion des prédictions en réels\n","#             predictions_real = torch.abs(predictions)\n","\n","#             # Calcul de la perte\n","#             loss = self.criterion(predictions_real, hrtfs_real)\n","#             loss = loss / accumulation_steps\n","#             loss.backward()\n","\n","#             if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n","#                 self.optimizer.step()\n","#                 self.optimizer.zero_grad()\n","\n","#             total_loss += loss.item()\n","#             del images, hrtfs, predictions\n","#             gc.collect()\n","#             torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)\n","\n","#     def validate(self, dataloader):\n","#         self.model.eval()\n","#         total_loss = 0\n","#         with torch.no_grad():\n","#             for images, hrtfs in dataloader:\n","#                 images = images.to(self.device).float()\n","#                 hrtfs = hrtfs.to(self.device)\n","\n","#                 # Convertir les données complexes en réels\n","#                 hrtfs_real = torch.abs(hrtfs)  # Utilisation de la magnitude\n","\n","#                 predictions = self.model(images)\n","\n","#                 # Conversion des prédictions en réels\n","#                 predictions_real = torch.abs(predictions)\n","\n","#                 # Calcul de la perte\n","#                 loss = self.criterion(predictions_real, hrtfs_real)\n","#                 total_loss += loss.item()\n","\n","#                 del images, hrtfs, predictions\n","#                 gc.collect()\n","#                 torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)\n","\n","# class HRTFTrainer:\n","#     def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","#         self.model = model.to(device)\n","#         self.device = device\n","#         self.criterion = nn.MSELoss()\n","#         self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","#     def train_epoch(self, dataloader, accumulation_steps=4):\n","#         self.model.train()\n","#         total_loss = 0\n","#         for i, (images, hrtfs) in enumerate(dataloader):\n","#             images = images.to(self.device).float()\n","#             hrtfs = hrtfs.to(self.device)\n","#             self.optimizer.zero_grad()\n","\n","#             predictions = self.model(images)\n","#             loss = self.criterion(predictions, hrtfs)\n","#             loss = loss / accumulation_steps\n","#             loss.backward()\n","\n","#             if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n","#                 self.optimizer.step()\n","#                 self.optimizer.zero_grad()\n","\n","#             total_loss += loss.item()\n","#             del images, hrtfs, predictions\n","#             gc.collect()\n","#             torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)\n","\n","#     def validate(self, dataloader):\n","#         self.model.eval()\n","#         total_loss = 0\n","#         with torch.no_grad():\n","#             for images, hrtfs in dataloader:\n","#                 images = images.to(self.device).float()\n","#                 hrtfs = hrtfs.to(self.device)\n","#                 predictions = self.model(images)\n","#                 loss = self.criterion(predictions, hrtfs)\n","#                 total_loss += loss.item()\n","#                 del images, hrtfs, predictions\n","#                 gc.collect()\n","#                 torch.cuda.empty_cache()\n","#         return total_loss / len(dataloader)"],"metadata":{"id":"aR2Cs_VG5iVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class HRTFGenerator(nn.Module):\n","#     def __init__(self, num_angles=19, num_freq_bins=129):\n","#         super().__init__()\n","#         self.encoder = PinnaEncoder()\n","\n","#         # Taille fictive initiale, à ajuster dynamiquement\n","#         self.flatten_size = None\n","\n","#         # Couches fully connected (on ajuste après le premier forward pass)\n","#         self.fc1 = None\n","#         self.fc2 = nn.Linear(512, 256)\n","#         self.fc3 = nn.Linear(256, num_angles * num_freq_bins * 2)  # *2 pour gauche et droite\n","#         self.dropout = nn.Dropout(0.5)\n","\n","#     def forward(self, images):\n","#         images = images.squeeze(3)  # Supprimer une dimension inutile si nécessaire\n","\n","#         batch_size, num_ears, num_views, height, width = images.shape\n","#         features = []\n","#         for ear in range(num_ears):\n","#             ear_features = []\n","#             for view in range(num_views):\n","#                 x = images[:, ear, view, :, :].unsqueeze(1)\n","#                 x = self.encoder(x)\n","#                 ear_features.append(x)\n","#             ear_features = torch.stack(ear_features, dim=1)\n","#             ear_features = torch.mean(ear_features, dim=1)\n","#             features.append(ear_features)\n","#         features = torch.cat(features, dim=1)\n","#         print(f\"Flatten size (features): {features.view(batch_size, -1).shape[1]}\")\n","\n","#         # Déterminer dynamiquement la taille\n","#         if self.flatten_size is None:\n","#             self.flatten_size = features.view(batch_size, -1).shape[1]\n","#             self.fc1 = nn.Linear(self.flatten_size, 512).to(features.device)\n","\n","#         features = features.view(batch_size, -1)\n","#         x = F.relu(self.fc1(features))\n","#         x = self.dropout(x)\n","#         x = F.relu(self.fc2(x))\n","#         x = self.dropout(x)\n","#         x = self.fc3(x)\n","#         hrtf = x.view(batch_size, 19, 2, 129)  # Ajustement de la sortie\n","#         return hrtf\n","\n","# class HRTFGenerator(nn.Module):\n","#     def __init__(self, num_angles=19, num_freq_bins=129):\n","#         super().__init__()\n","#         self.encoder = PinnaEncoder()\n","#         self.fc1 = nn.Linear(128 * 16 * 16, 512)  # Taille ajustée pour correspondre à l'encodeur\n","#         self.fc2 = nn.Linear(512, 256)\n","#         self.fc3 = nn.Linear(256, num_angles * num_freq_bins * 2)  # *2 pour gauche et droite\n","#         self.dropout = nn.Dropout(0.5)\n","\n","#     def forward(self, images):\n","#         # Suppression de la dimension inutile\n","#         images = images.squeeze(3)\n","\n","#         batch_size, num_ears, num_views, height, width = images.shape\n","#         features = []\n","#         for ear in range(num_ears):\n","#             ear_features = []\n","#             for view in range(num_views):\n","#                 x = images[:, ear, view, :, :].unsqueeze(1)\n","#                 x = self.encoder(x)\n","#                 ear_features.append(x)\n","#             ear_features = torch.stack(ear_features, dim=1)\n","#             ear_features = torch.mean(ear_features, dim=1)\n","#             features.append(ear_features)\n","#         features = torch.cat(features, dim=1)\n","#         features = features.view(batch_size, -1)\n","#         x = F.relu(self.fc1(features))\n","#         x = self.dropout(x)\n","#         x = F.relu(self.fc2(x))\n","#         x = self.dropout(x)\n","#         x = self.fc3(x)\n","#         hrtf = x.view(batch_size, 19, 2, 129)  # Ajustement de la sortie\n","#         return hr\n","# Entraîneur"],"metadata":{"id":"cqNlmxJb5xfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import gc\n","\n","# class PinnaEncoder(nn.Module):\n","#     def __init__(self):\n","#         super().__init__()\n","#         # CNN layers for processing pinna images\n","#         self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","#         self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","#         self.pool = nn.MaxPool2d(2, 2)\n","#         self.dropout = nn.Dropout(0.25)\n","\n","#     def forward(self, x):\n","#         x = self.pool(F.relu(self.conv1(x)))\n","#         x = self.dropout(x)\n","#         x = self.pool(F.relu(self.conv2(x)))\n","#         x = self.dropout(x)\n","#         x = self.pool(F.relu(self.conv3(x)))\n","#         x = self.dropout(x)\n","#         return x\n","\n","# class HRTFGenerator(nn.Module):\n","#     def __init__(self, num_angles=19, num_freq_bins=129):\n","#         super().__init__()\n","#         self.encoder = PinnaEncoder()\n","\n","#         # Calculate flattened size after convolutions\n","#         self.flatten_size = 128 * 16 * 16  # Adjust based on input image size\n","\n","#         # Fully connected layers\n","#         self.fc1 = nn.Linear(self.flatten_size, 1024)\n","#         self.fc2 = nn.Linear(1024, 512)\n","#         self.fc3 = nn.Linear(512, num_angles * num_freq_bins * 2)  # *2 for left and right channels\n","\n","#         self.dropout = nn.Dropout(0.5)\n","\n","#     def forward(self, images):\n","#         batch_size, num_ears, num_views, height, width = images.shape\n","\n","#         # Process each ear and view separately\n","#         features = []\n","#         for ear in range(num_ears):\n","#             ear_features = []\n","#             for view in range(num_views):\n","#                 x = images[:, ear, view, :, :].unsqueeze(1)  # Add channel dimension\n","#                 x = self.encoder(x)\n","#                 ear_features.append(x)\n","\n","#             # Combine features from different views\n","#             ear_features = torch.stack(ear_features, dim=1)\n","#             ear_features = torch.mean(ear_features, dim=1)  # Average pooling across views\n","#             features.append(ear_features)\n","\n","#         # Combine features from both ears\n","#         features = torch.cat(features, dim=1)\n","\n","#         # Flatten and pass through fully connected layers\n","#         x = features.view(batch_size, -1)\n","#         x = F.relu(self.fc1(x))\n","#         x = self.dropout(x)\n","#         x = F.relu(self.fc2(x))\n","#         x = self.dropout(x)\n","#         x = self.fc3(x)\n","\n","#         # Reshape output to match HRTF format\n","#         hrtf = x.view(batch_size, -1, 2, 129)  # (batch_size, num_angles, 2, num_freq_bins)\n","\n","#         return hrtf\n","\n","# class HRTFTrainer:\n","#     def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","#         self.model = model.to(device)\n","#         self.device = device\n","#         self.criterion = nn.MSELoss()\n","#         self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","#     def train_epoch(self, dataloader):\n","#         self.model.train()\n","#         total_loss = 0\n","\n","#         for images, hrtfs in dataloader:\n","#             images = images.to(self.device)\n","#             hrtfs = hrtfs.to(self.device)\n","\n","#             self.optimizer.zero_grad()\n","#             images = images.float()\n","#             predictions = self.model(images)\n","#             loss = self.criterion(predictions, hrtfs)\n","\n","#             loss.backward()\n","#             self.optimizer.step()\n","\n","#             total_loss += loss.item()\n","\n","#             del images, hrtfs, predictions, loss  # Delete batch-specific variables\n","#             gc.collect()  # Collect garbage\n","#             torch.cuda.empty_cache()  # Free up GPU memory\n","\n","#         return total_loss / len(dataloader)\n","\n","#     def validate(self, dataloader):\n","#         self.model.eval()\n","#         total_loss = 0\n","\n","#         with torch.no_grad():\n","#             for images, hrtfs in dataloader:\n","#                 images = images.to(self.device)\n","#                 hrtfs = hrtfs.to(self.device)\n","\n","#                 predictions = self.model(images)\n","#                 loss = self.criterion(predictions, hrtfs)\n","#                 total_loss += loss.item()\n","\n","#                 del images, hrtfs, predictions, loss  # Delete batch-specific variables\n","#                 gc.collect()  # Collect garbage\n","#                 torch.cuda.empty_cache()  # Free up GPU memory\n","\n","#         return total_loss / len(dataloader)\n","\n","# def train_model(train_loader, val_loader, num_epochs=50, batch_size=1):\n","#     model = HRTFGenerator()\n","#     trainer = HRTFTrainer(model)\n","\n","#     best_val_loss = float('inf')\n","#     for epoch in range(num_epochs):\n","#         print(f'Epoch {epoch+1}/{num_epochs}:')\n","#         train_loss = trainer.train_epoch(train_loader)\n","#         val_loss = trainer.validate(val_loader)\n","\n","#         if val_loss < best_val_loss:\n","#             best_val_loss = val_loss\n","#             torch.save(model.state_dict(), 'best_model.pth')\n","\n","#         print(f'Epoch {epoch+1}/{num_epochs}:')\n","#         print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","#         gc.collect()\n","#         torch.cuda.empty_cache()\n","\n","#     return model"],"metadata":{"id":"z5YVGCfk59tz"},"execution_count":null,"outputs":[]}]}